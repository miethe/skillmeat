name: Comprehensive Test Matrix

on:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main, develop]
  workflow_dispatch:

env:
  PYTHON_VERSION_DEFAULT: '3.11'
  NODE_VERSION_DEFAULT: '20'

jobs:
  # ============================================================================
  # Python Backend Tests - Cross-platform, Multi-version
  # ============================================================================
  python-tests:
    name: Python ${{ matrix.python-version }} - ${{ matrix.os }}
    runs-on: ${{ matrix.os }}
    strategy:
      fail-fast: false
      matrix:
        os: [ubuntu-latest, macos-latest, windows-latest]
        python-version: ['3.9', '3.10', '3.11', '3.12']

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v5
        with:
          python-version: ${{ matrix.python-version }}
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -e ".[dev]"

      - name: Run linting
        run: |
          flake8 skillmeat --count --select=E9,F63,F7,F82 --show-source --statistics
        continue-on-error: true

      - name: Run type checking
        run: mypy skillmeat --ignore-missing-imports
        continue-on-error: true

      - name: Run unit tests
        run: pytest -v -m "unit" --cov=skillmeat --cov-report=xml --cov-report=term
        continue-on-error: false

      - name: Run integration tests
        run: pytest -v -m "integration" --cov=skillmeat --cov-append --cov-report=xml --cov-report=term
        continue-on-error: false

      - name: Upload coverage to Codecov
        uses: codecov/codecov-action@v4
        with:
          file: ./coverage.xml
          flags: python-${{ matrix.os }}-py${{ matrix.python-version }}
          name: python-${{ matrix.os }}-${{ matrix.python-version }}
          fail_ci_if_error: false
        env:
          CODECOV_TOKEN: ${{ secrets.CODECOV_TOKEN }}

  # ============================================================================
  # Frontend Unit Tests - Jest
  # ============================================================================
  frontend-unit-tests:
    name: Frontend Unit - ${{ matrix.os }} - Node ${{ matrix.node-version }}
    runs-on: ${{ matrix.os }}
    strategy:
      fail-fast: false
      matrix:
        os: [ubuntu-latest, macos-latest, windows-latest]
        node-version: [18, 20]

    defaults:
      run:
        working-directory: skillmeat/web

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup pnpm
        uses: pnpm/action-setup@v4
        with:
          version: 8.15.0

      - name: Setup Node.js ${{ matrix.node-version }}
        uses: actions/setup-node@v4
        with:
          node-version: ${{ matrix.node-version }}
          cache: 'pnpm'
          cache-dependency-path: skillmeat/web/pnpm-lock.yaml

      - name: Install dependencies
        run: pnpm install --frozen-lockfile

      - name: Run Jest unit tests
        run: pnpm test:coverage

      - name: Upload coverage to Codecov
        uses: codecov/codecov-action@v4
        with:
          files: ./skillmeat/web/coverage/lcov.info
          flags: frontend-${{ matrix.os }}-node${{ matrix.node-version }}
          name: frontend-${{ matrix.os }}-${{ matrix.node-version }}
          fail_ci_if_error: false
        env:
          CODECOV_TOKEN: ${{ secrets.CODECOV_TOKEN }}

  # ============================================================================
  # Frontend E2E Tests - Playwright (Cross-browser)
  # ============================================================================
  frontend-e2e-tests:
    name: E2E - ${{ matrix.browser }}
    runs-on: ubuntu-latest
    timeout-minutes: 30
    strategy:
      fail-fast: false
      matrix:
        browser: [chromium, firefox, webkit]

    defaults:
      run:
        working-directory: skillmeat/web

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup pnpm
        uses: pnpm/action-setup@v4
        with:
          version: 8.15.0

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION_DEFAULT }}
          cache: 'pnpm'
          cache-dependency-path: skillmeat/web/pnpm-lock.yaml

      - name: Install dependencies
        run: pnpm install --frozen-lockfile

      - name: Install Playwright browsers
        run: pnpm exec playwright install --with-deps ${{ matrix.browser }}

      - name: Run E2E tests
        run: pnpm test:e2e --project=${{ matrix.browser }}
        env:
          CI: true

      - name: Upload test results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: e2e-results-${{ matrix.browser }}
          path: skillmeat/web/test-results/
          retention-days: 30

      - name: Upload Playwright report
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: playwright-report-${{ matrix.browser }}
          path: skillmeat/web/playwright-report/
          retention-days: 30

  # ============================================================================
  # Accessibility Tests
  # ============================================================================
  accessibility-tests:
    name: Accessibility Tests
    runs-on: ubuntu-latest
    timeout-minutes: 15

    defaults:
      run:
        working-directory: skillmeat/web

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup pnpm
        uses: pnpm/action-setup@v4
        with:
          version: 8.15.0

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION_DEFAULT }}
          cache: 'pnpm'
          cache-dependency-path: skillmeat/web/pnpm-lock.yaml

      - name: Install dependencies
        run: pnpm install --frozen-lockfile

      - name: Install Playwright browsers
        run: pnpm exec playwright install --with-deps chromium

      - name: Run accessibility tests
        run: pnpm test:a11y
        env:
          CI: true

      - name: Upload accessibility results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: accessibility-results
          path: skillmeat/web/test-results/
          retention-days: 30

  # ============================================================================
  # Integration Tests - Full Stack (Python + Frontend)
  # ============================================================================
  integration-full-stack:
    name: Full Stack Integration
    runs-on: ubuntu-latest
    timeout-minutes: 20

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION_DEFAULT }}
          cache: 'pip'

      - name: Setup pnpm
        uses: pnpm/action-setup@v4
        with:
          version: 8.15.0

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION_DEFAULT }}
          cache: 'pnpm'
          cache-dependency-path: skillmeat/web/pnpm-lock.yaml

      - name: Install Python dependencies
        run: pip install -e ".[dev]"

      - name: Install Node dependencies
        working-directory: skillmeat/web
        run: pnpm install --frozen-lockfile

      - name: Start API server in background
        run: |
          python -m skillmeat.api.server &
          echo $! > api_server.pid
          sleep 5
        env:
          SKILLMEAT_API_HOST: 0.0.0.0
          SKILLMEAT_API_PORT: 8000

      - name: Wait for API to be ready
        run: |
          for i in {1..30}; do
            if curl -f http://localhost:8000/health > /dev/null 2>&1; then
              echo "API server is ready"
              exit 0
            fi
            echo "Waiting for API server... ($i/30)"
            sleep 2
          done
          echo "API server failed to start"
          exit 1

      - name: Run integration tests
        run: pytest -v -m "integration or api" --cov=skillmeat --cov-report=xml

      - name: Stop API server
        if: always()
        run: |
          if [ -f api_server.pid ]; then
            kill $(cat api_server.pid) || true
          fi
          pkill -f "python -m skillmeat.api.server" || true

      - name: Upload coverage
        uses: codecov/codecov-action@v4
        with:
          file: ./coverage.xml
          flags: integration-full-stack
          name: integration-full-stack
          fail_ci_if_error: false
        env:
          CODECOV_TOKEN: ${{ secrets.CODECOV_TOKEN }}

  # ============================================================================
  # Security and Compliance Tests
  # ============================================================================
  security-tests:
    name: Security & Compliance
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION_DEFAULT }}
          cache: 'pip'

      - name: Install dependencies
        run: pip install -e ".[dev]"

      - name: Run security tests
        run: pytest -v -m "security or compliance" --tb=short
        continue-on-error: true

  # ============================================================================
  # Performance Tests
  # ============================================================================
  performance-tests:
    name: Performance Benchmarks
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION_DEFAULT }}
          cache: 'pip'

      - name: Install dependencies
        run: pip install -e ".[dev]"

      - name: Run performance tests
        run: pytest -v -m "performance" tests/performance/ --benchmark-only
        continue-on-error: true

  # ============================================================================
  # Test Report Summary
  # ============================================================================
  test-report:
    name: Test Report Summary
    runs-on: ubuntu-latest
    needs:
      - python-tests
      - frontend-unit-tests
      - frontend-e2e-tests
      - accessibility-tests
      - integration-full-stack
    if: always()

    steps:
      - name: Download all artifacts
        uses: actions/download-artifact@v4
        with:
          path: test-results

      - name: Generate summary report
        run: |
          echo "# Test Matrix Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "## Test Suite Status" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          echo "### Python Tests" >> $GITHUB_STEP_SUMMARY
          echo "${{ needs.python-tests.result == 'success' && '✅' || '❌' }} Python backend tests" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          echo "### Frontend Tests" >> $GITHUB_STEP_SUMMARY
          echo "${{ needs.frontend-unit-tests.result == 'success' && '✅' || '❌' }} Jest unit tests" >> $GITHUB_STEP_SUMMARY
          echo "${{ needs.frontend-e2e-tests.result == 'success' && '✅' || '❌' }} Playwright E2E tests" >> $GITHUB_STEP_SUMMARY
          echo "${{ needs.accessibility-tests.result == 'success' && '✅' || '❌' }} Accessibility tests" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          echo "### Integration Tests" >> $GITHUB_STEP_SUMMARY
          echo "${{ needs.integration-full-stack.result == 'success' && '✅' || '❌' }} Full stack integration" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          echo "## Coverage Reports" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "Coverage reports have been uploaded to Codecov and are available in the workflow artifacts." >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "## Test Artifacts" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "Test results, screenshots, and reports are available in the Actions artifacts section." >> $GITHUB_STEP_SUMMARY

      - name: Check overall status
        run: |
          if [ "${{ needs.python-tests.result }}" != "success" ] || \
             [ "${{ needs.frontend-unit-tests.result }}" != "success" ] || \
             [ "${{ needs.frontend-e2e-tests.result }}" != "success" ] || \
             [ "${{ needs.integration-full-stack.result }}" != "success" ]; then
            echo "Some tests failed. Please review the results above."
            exit 1
          fi
          echo "All critical tests passed!"
