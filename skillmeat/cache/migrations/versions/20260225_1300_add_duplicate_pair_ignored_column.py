"""Add duplicate_pairs table with ignored column for similar-artifacts feature.

Revision ID: 20260225_1300_add_duplicate_pair_ignored_column
Revises: 20260225_1200_add_color_icon_to_deployment_sets
Create Date: 2026-02-25 13:00:00.000000+00:00

Background
----------
Part of the similar-artifacts-v1 feature (SA-P1-001).  The ``DuplicatePair``
ORM model persists pairwise similarity results between collection artifacts so
that:

  1. Expensive O(n²) comparisons do not need to be repeated on every request.
  2. Users can dismiss false-positive suggestions via the ``ignored`` flag
     without losing the underlying score data.

Table created
-------------
``duplicate_pairs``
    id            String PK   — UUID hex identifier
    artifact1_uuid  String    — UUID of the first artifact
    artifact2_uuid  String    — UUID of the second artifact
    similarity_score  Float   — normalised score in [0.0, 1.0]
    match_reasons   Text      — JSON-encoded list of reason strings, default '[]'
    ignored         Boolean   — dismissal flag, NOT NULL, default FALSE (0)
    created_at      DateTime  — record creation timestamp
    updated_at      DateTime  — last-update timestamp

Constraints
-----------
``check_duplicate_pair_score``
    Enforces 0.0 <= similarity_score <= 1.0.

``uq_duplicate_pair_artifacts``
    UNIQUE (artifact1_uuid, artifact2_uuid) — prevents duplicate insertions for
    the same ordered pair.

Indexes
-------
``idx_duplicate_pairs_artifact1``  — fast lookup by artifact1_uuid
``idx_duplicate_pairs_artifact2``  — fast lookup by artifact2_uuid
``idx_duplicate_pairs_ignored``    — efficient filter on the ignored flag

Rollback
--------
Drops the ``duplicate_pairs`` table entirely.  All stored similarity pairs will
be permanently lost on downgrade; they can be regenerated by re-running the
similarity engine.
"""

from __future__ import annotations

from typing import Sequence, Union

import sqlalchemy as sa
from alembic import op
from sqlalchemy import inspect as sa_inspect

# revision identifiers, used by Alembic.
revision: str = "20260225_1300_add_duplicate_pair_ignored_column"
down_revision: Union[str, None] = "20260225_1200_add_color_icon_to_deployment_sets"
branch_labels: Union[str, Sequence[str], None] = None
depends_on: Union[str, Sequence[str], None] = None

_TABLE = "duplicate_pairs"


def upgrade() -> None:
    """Create the duplicate_pairs table.

    The table is created only when it does not already exist so that the
    migration is idempotent in test environments where tables may have been
    created by ``Base.metadata.create_all()`` before Alembic ran.
    """
    bind = op.get_bind()
    inspector = sa_inspect(bind)

    if _TABLE in inspector.get_table_names():
        # Table already exists — ensure the ignored column is present.
        # This handles the case where create_all() ran before the migration.
        existing_cols = {col["name"] for col in inspector.get_columns(_TABLE)}
        if "ignored" not in existing_cols:
            with op.batch_alter_table(_TABLE) as batch_op:
                batch_op.add_column(
                    sa.Column(
                        "ignored",
                        sa.Boolean(),
                        nullable=False,
                        server_default="0",
                        comment="True when the user has dismissed this pair from the UI",
                    )
                )
        return

    op.create_table(
        _TABLE,
        sa.Column("id", sa.String(), primary_key=True),
        sa.Column(
            "artifact1_uuid",
            sa.String(32),
            nullable=False,
            comment="UUID of the first artifact in the similar pair",
        ),
        sa.Column(
            "artifact2_uuid",
            sa.String(32),
            nullable=False,
            comment="UUID of the second artifact in the similar pair",
        ),
        sa.Column(
            "similarity_score",
            sa.Float(),
            nullable=False,
            comment="Normalised similarity score in the range [0.0, 1.0]",
        ),
        sa.Column(
            "match_reasons",
            sa.Text(),
            nullable=True,
            server_default="[]",
            comment="JSON-encoded list of human-readable match reason strings",
        ),
        sa.Column(
            "ignored",
            sa.Boolean(),
            nullable=False,
            server_default="0",
            comment="True when the user has dismissed this pair from the UI",
        ),
        sa.Column(
            "created_at",
            sa.DateTime(),
            nullable=False,
        ),
        sa.Column(
            "updated_at",
            sa.DateTime(),
            nullable=False,
        ),
        sa.CheckConstraint(
            "similarity_score >= 0.0 AND similarity_score <= 1.0",
            name="check_duplicate_pair_score",
        ),
        sa.UniqueConstraint(
            "artifact1_uuid",
            "artifact2_uuid",
            name="uq_duplicate_pair_artifacts",
        ),
    )

    # Create indexes after table creation so they can be added concurrently on
    # PostgreSQL if this project ever migrates away from SQLite.
    op.create_index(
        "idx_duplicate_pairs_artifact1",
        _TABLE,
        ["artifact1_uuid"],
    )
    op.create_index(
        "idx_duplicate_pairs_artifact2",
        _TABLE,
        ["artifact2_uuid"],
    )
    op.create_index(
        "idx_duplicate_pairs_ignored",
        _TABLE,
        ["ignored"],
    )


def downgrade() -> None:
    """Drop the duplicate_pairs table.

    All stored similarity pairs will be permanently lost.  They can be
    regenerated by re-running the similarity engine after re-applying the
    upgrade migration.
    """
    bind = op.get_bind()
    inspector = sa_inspect(bind)

    if _TABLE not in inspector.get_table_names():
        return

    op.drop_index("idx_duplicate_pairs_ignored", table_name=_TABLE)
    op.drop_index("idx_duplicate_pairs_artifact2", table_name=_TABLE)
    op.drop_index("idx_duplicate_pairs_artifact1", table_name=_TABLE)
    op.drop_table(_TABLE)
